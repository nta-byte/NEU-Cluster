{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis- No Histogram Equalization\n",
    "\n",
    "In this notebook we will evaluate the clustering performance of the standard analysis, \n",
    "\n",
    "but replacing the features with the fc1 feature descriptors computed on images\n",
    "\n",
    "without histogram equalizatione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## module imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "import skimage\n",
    "import skimage.io\n",
    "from skimage.exposure import histogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from helper import classification_tools as ct\n",
    "from helper import visualize as vis\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Once again we just need the feature data and label encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = Path('..','data','features','VGG16_fc1_features_NoHistEQ.pickle')\n",
    "assert feature_path.is_file()\n",
    "\n",
    "le_path = Path('..','models','label_encoder.pickle')\n",
    "assert le_path.is_file()\n",
    "\n",
    "\n",
    "with open(feature_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open(le_path, 'rb') as f:\n",
    "    le = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = data['filename']\n",
    "fc1 = data['features']\n",
    "labels = data['labels']\n",
    "y_gt = le.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction\n",
    "## PCA\n",
    "Without histogram equalization the PCA components will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_n = PCA(svd_solver='full')\n",
    "x_pca_ = pca_n.fit_transform(fc1)\n",
    "\n",
    "var_ = pca_n.explained_variance_ratio_.cumsum()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,3),dpi=150,)\n",
    "ax.grid('on', which='both')\n",
    "ax.plot(range(1,len(var_)+1), var_)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('number of components')\n",
    "ax.set_ylabel('cumulative fraction of\\nvariance explained')\n",
    "print('variance preserved by 50 components: {:.3f}'.format(var_[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, 50 components only preserved 74% of the variance. Removing histogram equalization actually increases the amount of variance explained by the first 50 components, as the relative brightness of features is preserved in the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50, svd_solver='full', whiten=True)\n",
    "pca_nw = PCA(n_components=50, svd_solver='full', whiten=False)\n",
    "x = pca.fit_transform(fc1)\n",
    "x_nw = pca_nw.fit_transform(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=605196619)\n",
    "tsne_w = TSNE(n_components=2, random_state=3618296892)\n",
    "x_nw_tsne = tsne.fit_transform(x_nw)\n",
    "x_w_tsne = tsne_w.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'files': files,\n",
    "                   'x_nw':x_nw_tsne[:,0],\n",
    "                   'y_nw':x_nw_tsne[:,1],\n",
    "                   'x_w': x_w_tsne[:,0],\n",
    "                  'y_w': x_w_tsne[:,1],\n",
    "                   'labels': labels,\n",
    "                  },\n",
    "                  index=files)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(6,3), dpi=150)\n",
    "sns.scatterplot(data=df, x='x_nw', y='y_nw', hue='labels', hue_order=le.labels_ordered,  ax=ax[0])\n",
    "sns.scatterplot(data=df, x='x_w', y='y_w', hue='labels', hue_order=le.labels_ordered, ax=ax[1])\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].legend(bbox_to_anchor=(1.05,1))\n",
    "ax[0].set_title('t-sne/pca\\n without whitening')\n",
    "ax[1].set_title('t-sne/pca with whitening')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-sne clusters still look good for unwhitened components. Whitening causes the clusters to move closer together on t-sne, the same as before. Notice how rolled-in scale (RS) now has two separate clusters, similar to scratches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.pano_plot(x_nw_tsne[:,0], x_nw_tsne[:,1], files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without histogram equalization, we see the separation of some clusters by brightness. Note the two distinct clusters for RS samples that was not present before. One cluster has brighter stripes from the illumination of the images, while the other has a darker background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "First, we will use the same settings as before, to see the effect of removing histogram equalization on the standard analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8, init='k-means++', \n",
    "                n_init=500,random_state=3831647997)\n",
    "kmeans.fit(x)\n",
    "labels_unmatched = kmeans.labels_  # labels from k-means are arbitrary and must be mapped to the format of the ground truth labels\n",
    "y_pred = ct.label_matcher(labels_unmatched, y_gt,)  # this function matches the labels so we can compare directly\n",
    "print('inertia: {:.2f}'.format(kmeans.inertia_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_str = le.inverse_transform(y_pred)\n",
    "df['y_pred_labels'] = pd.Series(y_pred_str, index=files)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,5), dpi=150)\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x='x_nw', y='y_nw', hue='labels', hue_order=le.labels_ordered, ax=ax[0]) # ground truth labels\n",
    "sns.scatterplot(data=df, x='x_nw', y='y_nw', hue='y_pred_labels', hue_order=le.labels_ordered, ax=ax[1]) # predicted labels\n",
    "\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].legend(bbox_to_anchor=(1.05,1))\n",
    "ax[0].set_title('hue: ground truth labels')\n",
    "ax[1].set_title('hue: predicted labels')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see some significant differences between the labels. For instance- scratches (brown) being classified as inclusions (orange.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confusion matrix allows us to evaluate the performance\n",
    "CM = confusion_matrix(y_gt, y_pred)\n",
    "fig, ax = plt.subplots(dpi=300, figsize=(5,5))\n",
    "vis.pretty_cm(CM, le.labels_ordered, ax0=ax, fs=12)\n",
    "print('Accuracy: {:.3f}'.format(CM.trace()/CM.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the accuracy drops from above 99% to 93%, a significant increase in the number of misclassifications. This is driven by a large number of incorrect predictions for images of scratches.\n",
    "\n",
    "Again, just to be sure we didn't get unlucky with the random seed, let's run it 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a couple minutes to run, be patient!\n",
    "nohisteq_10run_cache_path = Path('..','.neu_cache','nohisteq_10runs.pickle')\n",
    "use_cache=True\n",
    "\n",
    "if not use_cache or not nohisteq_10run_cache_path.is_file():\n",
    "    rs = np.random.RandomState(seed=987654321)\n",
    "    accuracies = np.zeros(10)\n",
    "    for i, seed in enumerate(rs.randint(2**32, size=10)):\n",
    "        kmeans_ = KMeans(n_clusters=7, init='k-means++', n_init=500,random_state=seed)\n",
    "        kmeans_.fit(x)\n",
    "        labels_unmatched_ = kmeans_.labels_\n",
    "        y_pred_ = ct.label_matcher(labels_unmatched_, y_gt)\n",
    "        accuracies[i] = (y_pred_ == y_gt).sum()/len(y_gt)\n",
    "    \n",
    "    with open(nohisteq_10run_cache_path, 'wb') as f:\n",
    "        pickle.dump(accuracies, f)\n",
    "else:\n",
    "    with open(nohisteq_10run_cache_path, 'rb') as f:\n",
    "        accuracies = pickle.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10 runs:\\n\\tavg: {:.4f}\\n\\tstd: {:.4},\\n\\tmin: {:.4f}'.format(accuracies.mean(), np.std(accuracies), accuracies.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we do better?\n",
    "From the t-sne map above, we can see that RS splits into 2 clusters. Let's adjust the number of clusters and see how this impacts the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nohisteq_accvsk_cache_path = Path('..','.neu_cache','nohisteq_accvsk.pickle')\n",
    "use_cache = True\n",
    "\n",
    "if not use_cache or not nohisteq_accvsk_cache_path.is_file():\n",
    "    rs = np.random.RandomState(seed=723271629)\n",
    "    k_values = np.arange(7,16) \n",
    "    accuracies_k = np.zeros(len(k_values))\n",
    "\n",
    "    for i, (k, state) in enumerate(zip(k_values, rs.randint(2**32, size=len(k_values)))):\n",
    "        kmeans_ = KMeans(n_clusters=k, init='k-means++', n_init=500,\n",
    "                         random_state=state)\n",
    "        kmeans_.fit(x)\n",
    "        labels_unmatched_ = kmeans_.labels_\n",
    "        y_pred_ = ct.label_matcher(labels_unmatched_, y_gt)\n",
    "        accuracies_k[i] = (y_pred_ == y_gt).sum()/len(y_gt)\n",
    "    \n",
    "    with open(nohisteq_accvsk_cache_path, 'wb') as f:\n",
    "        pickle.dump({'k_values': k_values, 'accuracies_k': accuracies_k}, f)\n",
    "else:\n",
    "    with open(nohisteq_accvsk_cache_path, 'rb') as f:\n",
    "        results_ = pickle.load(f)\n",
    "        k_values = results_['k_values']\n",
    "        accuracies_k = results_['accuracies_k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(k_values, accuracies_k)\n",
    "ax.set_xlabel('number of clusters')\n",
    "ax.set_ylabel('classification accuracy')\n",
    "plt.show()\n",
    "print('max accuracy: {:.3f}\\nnum clusters: {}'.format(accuracies_k.max(),\n",
    "                                        k_values[accuracies_k.argmax()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing k from 7 to 12 brings the accuracy up to almost 0.95, but this is still much lower than the 99.4 achieved with histogram equalization (it makes almost 10x as many mistakes). \n",
    "\n",
    "Capturing differences in brightness causes different defects to cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures\n",
    "At this point this part of the analysis is done, I just need to generate some figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the t-sne plot, the two RS clusters can separate based on their y-value \n",
    "# y = 20 perfectly separates the clusters\n",
    "\n",
    "rs_label = le.transform(['RS'])[0]\n",
    "RSgroup1 = np.logical_and(x_nw_tsne[:,1] > 20, \n",
    "                          y_gt==rs_label)\n",
    "RSgroup2 = np.logical_and(x_nw_tsne[:,1] < 20, \n",
    "                          y_gt==rs_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the 'most typical' image from each cluster, we can select the\n",
    "# point that is closest to the cluster centroid\n",
    "\n",
    "x1_Rs1 = x_nw_tsne[RSgroup1]\n",
    "x1_Rs2 = x_nw_tsne[RSgroup2]\n",
    "\n",
    "i1 = cdist(x1_Rs1, \n",
    "        np.mean(x1_Rs1, axis=0)[np.newaxis, :]).argmin()\n",
    "i2 = cdist(x1_Rs2, \n",
    "        np.mean(x1_Rs2, axis=0)[np.newaxis, :]).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labarray = np.asarray(labels)\n",
    "filearray = np.asarray(files)\n",
    "\n",
    "group1files = filearray[RSgroup1]\n",
    "group2files = filearray[RSgroup2]\n",
    "\n",
    "file1 = group1files[i1]\n",
    "file2 = group2files[i2]\n",
    "\n",
    "im1 = skimage.io.imread(file1)\n",
    "im2 = skimage.io.imread(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(6,6), dpi=300)\n",
    "gs = gridspec.GridSpec(nrows=2, ncols=2)\n",
    "ax1 = figure.add_subplot(gs[0,:])\n",
    "ax2 = figure.add_subplot(gs[1,0])\n",
    "ax3 = figure.add_subplot(gs[1,1])\n",
    "sns.scatterplot(x_nw_tsne[:,0], x_nw_tsne[:,1], \n",
    "                 hue=labels, hue_order=le.labels_ordered, ax=ax1)\n",
    "l1 = ax1.legend(loc='upper center', ncol=6,\n",
    "                bbox_to_anchor=(0.55,1.18), columnspacing=1)\n",
    "ax1.plot(x1_Rs1[i1,0], x1_Rs1[i1,1], 'kd')\n",
    "ax1.plot(x1_Rs2[i2,0], x1_Rs2[i2,1], 'ks')\n",
    "\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "h1, b1 = histogram(im1)\n",
    "h2, b2 = histogram(im2)\n",
    "\n",
    "hmax = max(h1.max(), h2.max())\n",
    "\n",
    "ax2.imshow(im1, cmap='gray', extent=(0,1,0,1))\n",
    "ax2.plot(b1/255, h1/hmax, 'r')\n",
    "ax2.set_xlabel('intensity')\n",
    "ax2.set_ylabel('frequency')\n",
    "ax3.imshow(im2, cmap='gray', extent=(0,1,0,1))\n",
    "ax3.plot(b2/255, h2/hmax, 'r')\n",
    "ax3.set_yticklabels('')\n",
    "ax3.set_xlabel('intensity')\n",
    "\n",
    "bb = ax1.get_position()\n",
    "ax2.set_position((0.125, *ax2.get_position().bounds[1:]))\n",
    "\n",
    "\n",
    "ax1.set_title('A)', loc='left')\n",
    "ax2.set_title('B)', loc='left')\n",
    "ax3.set_title('C)', loc='left')\n",
    "fig.tight_layout()\n",
    "fig_path = Path('..','Figures')\n",
    "fig.savefig(fig_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=1341411652\n",
    "kmeans_ = KMeans(n_clusters=12, init='k-means++', n_init=500,\n",
    "                         random_state=state)\n",
    "kmeans_.fit(x)\n",
    "labels_unmatched_ = kmeans_.labels_\n",
    "y_pred_ = ct.label_matcher(labels_unmatched_, y_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_multi_ = ct.label_matcher_multicluster(labels_unmatched_, y_gt, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(b1/255)[h1.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(6,6), dpi=300, facecolor='w')\n",
    "gs = gridspec.GridSpec(nrows=3, ncols=2, hspace=0.25, figure=fig)\n",
    "ax1 = fig.add_subplot(gs[0,:])\n",
    "ax2 = fig.add_subplot(gs[1,:])\n",
    "ax3 = fig.add_subplot(gs[2,0])\n",
    "ax4 = fig.add_subplot(gs[2,1])\n",
    "\n",
    "\n",
    "\n",
    "sns.scatterplot(x_nw_tsne[:,0], x_nw_tsne[:,1], \n",
    "                 hue=labels, hue_order=le.labels_ordered, \n",
    "                ax=ax1,)\n",
    "l1 = ax1.legend(loc='upper center', ncol=2,\n",
    "                 bbox_to_anchor=(1.2,1.05), columnspacing=1)\n",
    "\n",
    "colors = np.concatenate([sns.color_palette(), sns.color_palette('dark', 2)], axis=0)\n",
    "sns.scatterplot(x_nw_tsne[:,0], x_nw_tsne[:,1], \n",
    "                 hue=y_pred_multi_, hue_order=sorted(np.unique(y_pred_multi_), key=lambda x : x.upper()), \n",
    "                ax=ax2, palette=list(colors))\n",
    "l2 = ax2.legend(loc='upper center', ncol=2,\n",
    "                bbox_to_anchor=(1.2,1.05), columnspacing=1)\n",
    "\n",
    "ax1.plot(x1_Rs1[i1,0], x1_Rs1[i1,1], 'kd')\n",
    "ax1.plot(x1_Rs2[i2,0], x1_Rs2[i2,1], 'ks')\n",
    "\n",
    "\n",
    "for a in [ax1, ax2]:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax3.imshow(im1, cmap='gray', extent=(0,1,0,1))\n",
    "ax3.plot(b1/255, h1/hmax, 'r')\n",
    "ax3.set_xlabel('intensity')\n",
    "ax3.set_ylabel('frequency')\n",
    "ax4.imshow(im2, cmap='gray', extent=(0,1,0,1))\n",
    "ax4.plot(b2/255, h2/hmax, 'r')\n",
    "ax4.set_yticklabels('')\n",
    "ax4.set_xlabel('intensity')\n",
    "\n",
    "\n",
    "ax1.set_title('a)', **{'x':0.05,'y':0.80})\n",
    "ax2.set_title('b)', **{'x':0.05,'y':0.80})\n",
    "ax3.set_title('c)', **{'x':0.15,'y':0.80, 'color':'k'})\n",
    "ax4.set_title('d)', **{'x':0.15,'y':0.80, 'color':'k'})\n",
    "\n",
    "fig.savefig(Path('..','Figures','tsne_nohisteq.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(b1/255)[h1.argmax()], (b2/255)[h2.argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_ = confusion_matrix(y_gt, y_pred_)\n",
    "fig, ax = plt.subplots(figsize=(3,3), dpi=300)\n",
    "vis.pretty_cm(cm_, le.labels_ordered, ax0=ax, cmap='cool', fs=8)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(Path('..','Figures','no_histeq_cm.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(cm_.trace()/cm_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ct.latex_report(y_gt, y_pred_, le.labels_ordered))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
