{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "We will use the VGG16 network as a signal processor, generating a feature descriptor for each image that we can use later for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the weights of the VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "vgg16_path = Path('..','models','VGG16.h5')\n",
    "if not vgg16_path.is_file():\n",
    "    vgg16 = keras.applications.VGG16(include_top=True,  # include fully connected layers\n",
    "                                     weights='imagenet') # use pre-trained model\n",
    "    vgg16.save(vgg16_path)\n",
    "    \n",
    "else:   \n",
    "    vgg16 = keras.models.load_model(vgg16_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the strutcure of the VGG16 model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained model will run data through the entire network and return the output of the classification layer. \n",
    "Howevever, we only want the output of the intermediate layer so that we can use it as a feature descriptor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_extractor(model=vgg16, layer='fc1'):\n",
    "    \"\"\"\n",
    "    returns a model that will extract the outputs of *layer* from *model*.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    model: keras model\n",
    "        full model from which intermediate layer will be extracted\n",
    "    layer: string\n",
    "        name of layer from which to extract outputs\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    new_model: keras model\n",
    "        feature extractor model which takes the same inputs as *model* and returns the outputs\n",
    "        of the intermediate layer specified by *layer* by calling new_model.predict(inputs)\n",
    "    \"\"\"\n",
    "    assert layer in [x.name for x in model.layers]\n",
    "    new_model = keras.Model(inputs = vgg16.input, outputs=[vgg16.get_layer(layer).output])\n",
    "    return new_model\n",
    "\n",
    "\n",
    "fc1_extractor = layer_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get the file paths of the pre-processed images we saved in 01_preprocess.ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = Path('..','data','images_preprocessed','images_histeq_resize')\n",
    "assert img_root.is_dir()\n",
    "files = sorted(img_root.glob(\"*.bmp\"))\n",
    "\n",
    "## Shuffle the filenames so they appear randomly in the dataset\n",
    "rs = np.random.RandomState(seed=749976)\n",
    "rs.shuffle(files)\n",
    "\n",
    "assert len(files) == 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature extraction to work correctly, the images have to be in the correct format for the network weights.\n",
    "Keras gives us functions for loading and formatting these images. Note the function is called 'preprocessing,'\n",
    "but it does not actually change the properties of the image like the preprocessing we did before. Instead, it \n",
    "ensures that the images are represented the correct way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "images = [image.load_img(file) for file in files] # load images\n",
    "images = np.asarray([image.img_to_array(img) for img in images]) # convert images to an array with shape consistent for the vgg16 input\n",
    "images = preprocess_input(images) # normalizes the pixel values to match the imagenet format (and therefore the pre-trained weights)\n",
    "print(images.shape) # 1800 images, 224x224 pixels, 3 color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract the features. This will take several minutes to run, there are a lot of operations to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 4096)\n"
     ]
    }
   ],
   "source": [
    "fc1 = fc1_extractor.predict(images)\n",
    "print(fc1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to extract the labels here as well. The labels are determined from the characteris in the filename before the first \"_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(f): return [x.stem.split('_')[0] for x in f]\n",
    "labels = extract_labels(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'filename' : files,\n",
    "           'features': fc1,\n",
    "          'labels': labels,\n",
    "           'layer_name': 'fc1'\n",
    "          }\n",
    "\n",
    "feature_dir = Path('..','data','features')\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "with open(feature_dir / 'VGG16_fc1_features_std.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding\n",
    "One last step that will make our lives easier throughout the analysis is standardizing the\n",
    "encoding of labels. The labels are stored as strings in the filenames, but it will be more\n",
    "convenient to convert them to numeric values so we can do math on them.\n",
    "We can create one LabelEncoder model and save it for reuse throughout the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_unique: ['Cr', 'In', 'Pa', 'PS', 'RS', 'Sc']\n",
      "first 10 integer labels: [3 4 5 1 0 0 4 1 0 4]\n",
      "first 10 string labels: ['Pa' 'RS' 'Sc' 'In' 'Cr' 'Cr' 'RS' 'In' 'Cr' 'RS']\n"
     ]
    }
   ],
   "source": [
    "labels_unique = sorted(np.unique(labels),\n",
    "                       key = lambda x: x.upper()) # removes effect of capitalization on sorting\n",
    "le = LabelEncoder()\n",
    "le.fit(labels_unique)\n",
    "labels_int = le.transform(labels[:10])\n",
    "labels_str = le.inverse_transform(labels_int)\n",
    "\n",
    "print('labels_unique: {}'.format(labels_unique))\n",
    "print('first 10 integer labels: {}'.format(labels_int))\n",
    "print('first 10 string labels: {}'.format(labels_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path('..','models','label_encoder.pickle'), 'wb') as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other feature representations\n",
    "As part of the sensitivity analysis we will need some other feature representations as well, to see how these changes impact the cluster analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No histogram equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root_nohisteq = Path('..','data','images_preprocessed','images_resize')\n",
    "assert img_root_nohisteq.is_dir()\n",
    "files_noh = sorted(img_root_nohisteq.glob('*'))\n",
    "\n",
    "rs = np.random.RandomState(seed=3626210179)\n",
    "\n",
    "rs.shuffle(files_noh)\n",
    "\n",
    "assert len(files_noh) == 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# follow the same process described above to load images, convert to array, and format for vgg16\n",
    "\n",
    "images_noh = [image.load_img(file) for file in files_noh]\n",
    "images_noh = np.asarray([image.img_to_array(img) for img in images_noh])\n",
    "images_noh = preprocess_input(images_noh)\n",
    "\n",
    "print(images_noh.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 4096)\n"
     ]
    }
   ],
   "source": [
    "fc1_extractor = layer_extractor()\n",
    "fc1_noh = fc1_extractor.predict(images_noh)\n",
    "\n",
    "print(fc1_noh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_noh = extract_labels(files_noh)\n",
    "results = {'filename': files_noh,\n",
    "           'features': fc1_noh,\n",
    "           'labels' : labels_noh,\n",
    "           'layer_name': 'fc1 (no histeq)'}\n",
    "\n",
    "feature_dir = Path('..','data','features')\n",
    "with open(feature_dir / 'VGG16_fc1_features_NoHistEQ.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
