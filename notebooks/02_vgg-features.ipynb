{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from helper.classification_tools import CustomLabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading files\n",
    "First, we need to get the file paths of the pre-processed images we saved in 01_preprocess.ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = Path('..','data','images_preprocessed','images_histeq_resize')\n",
    "assert img_root.is_dir()\n",
    "files = sorted(img_root.glob(\"*.bmp\"))\n",
    "\n",
    "## Shuffle the filenames so they appear randomly in the dataset\n",
    "rs = np.random.RandomState(seed=749976)\n",
    "rs.shuffle(files)\n",
    "\n",
    "assert len(files) == 1800\n",
    "print('first 10 filenames: {}'.format([x.name for x in files[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the labels from filenames\n",
    "The labels are determined from the characteris in the filename before the first \"_\". We could also just take the first two characters of the filename, but this does not generalize to cases where the labels have different numbers of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(f): return [x.stem.split('_')[0] for x in f]\n",
    "labels = extract_labels(files)\n",
    "print('first 10 labels: {}'.format(labels[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding\n",
    "One step that will make our lives easier throughout the analysis is standardizing the\n",
    "encoding of labels. The labels are stored as strings in the filenames, but it will be more\n",
    "convenient to convert them to numeric values for more convenient calculations of statistics like accuracy, precision, recall, etc.\n",
    "We can create one LabelEncoder model and save it for reuse throughout the study.\n",
    "\n",
    "sklearn has a LabelEncoder object, but it doesn't let you sort the labels alphabetically. Therefore I wrote a simple label encoder which can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = CustomLabelEncoder()\n",
    "le.fit(labels, sorter=lambda x: x.upper())\n",
    "\n",
    "labels_int = le.transform(labels[:10])\n",
    "labels_str = le.inverse_transform(labels_int)\n",
    "\n",
    "# save the label encoder so it can be used throughout the rest of this study\n",
    "with open(Path('..','models','label_encoder.pickle'), 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print('label encodings: {}'.format(le.mapper))\n",
    "print('first 10 integer labels: {}'.format(labels_int))\n",
    "print('first 10 string labels: {}'.format(labels_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Images\n",
    "For feature extraction to work correctly, the images have to be in the correct format for the network weights.\n",
    "Keras gives us functions for loading and formatting these images. Note the function is called 'preprocessing,'\n",
    "but it does not actually change the properties of the image like the preprocessing we did before. Instead, it \n",
    "ensures that the images are represented the correct way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(paths):\n",
    "    \"\"\"\n",
    "    Loads images in the correct format for use with the Keras VGG16 model\n",
    "    \n",
    "    Images are loaded as PIL image objects, converted to numpy array, and then formatted\n",
    "    with the appropriate VGG16.preprocess_input() function. Note that this only changes\n",
    "    how the images are represented, it does not change the actual visual properties of the\n",
    "    images like preprocessing did before.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    paths: list(Path)\n",
    "        list of Paths to each file where the image is stored. Note that the images should \n",
    "        have the same height, width in pixels so they can be stored in one array.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    images: ndarray\n",
    "        n_images x r x c x 3 array of pixel values that is compatible with the Keras model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    images = [image.load_img(file) for file in paths] # load images\n",
    "    # convert images to an array with shape consistent for the vgg16 input\n",
    "    images = np.asarray([image.img_to_array(img) for img in images]) \n",
    "    # normalizes the pixel values to match the imagenet format (and therefore the pre-trained weights)\n",
    "    images = preprocess_input(images) \n",
    "    \n",
    "    return images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images(files)\n",
    "assert len(images) == 1800\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "We will use the VGG16 network as a signal processor, generating a feature descriptor for each image that we can use later for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the weights of the VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_path = Path('..','models','VGG16.h5')\n",
    "if not vgg16_path.is_file():\n",
    "    vgg16 = keras.applications.VGG16(include_top=True,  # include fully connected layers\n",
    "                                     weights='imagenet') # use pre-trained model\n",
    "    vgg16.save(vgg16_path) # save model so we don't have to download it everytime\n",
    "    \n",
    "else:   \n",
    "    vgg16 = keras.models.load_model(vgg16_path) # use saved model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning indicates that the model hasn't been compiled with an optimizer/loss function for training. Since we are \n",
    "not training the model, and are just using it as a feature extractor, this is not a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the strutcure of the VGG16 model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained model will run data through the entire network and return the output of the classification layer. \n",
    "Howevever, we only want the output of the intermediate layer so that we can use it as a feature descriptor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_extractor(model=vgg16, layer='fc1'):\n",
    "    \"\"\"\n",
    "    returns a model that will extract the outputs of *layer* from *model*.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    model: keras model\n",
    "        full model from which intermediate layer will be extracted\n",
    "    layer: string\n",
    "        name of layer from which to extract outputs\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    new_model: keras model\n",
    "        feature extractor model which takes the same inputs as *model* and returns the outputs\n",
    "        of the intermediate layer specified by *layer* by calling new_model.predict(inputs)\n",
    "    \"\"\"\n",
    "    assert layer in [x.name for x in model.layers]  # make sure the layer exists\n",
    "\n",
    "    new_model = keras.Model(inputs = vgg16.input, outputs=[vgg16.get_layer(layer).output])\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC1 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_extractor = layer_extractor()\n",
    "fc1 = fc1_extractor.predict(images)\n",
    "\n",
    "# save results\n",
    "results = {'filename' : files,\n",
    "           'features': fc1,\n",
    "          'labels': labels,\n",
    "           'layer_name': 'fc1'\n",
    "          }\n",
    "\n",
    "feature_dir = Path('..','data','features')\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "with open(feature_dir / 'VGG16_fc1_features_std.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(fc1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features from other layers\n",
    "Simply repeat the process substituting the name of the layer you wish to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'block3_conv2' in [x.name for x in vgg16.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in ['fc2', 'block5_pool', 'block5_conv3']:\n",
    "    extractor = layer_extractor(layer=layer)  # model to extract features for each layer\n",
    "    features = extractor.predict(images)  # features extracted by model\n",
    "    # save the results using the same format as before\n",
    "    results = {'filename': files,\n",
    "              'features': features,\n",
    "              'labels': labels,\n",
    "              'layer_name': layer}\n",
    "    with open(feature_dir / 'VGG16_{}_features.pickle'.format(layer), 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC1 features without histogram equalization\n",
    "Use the fc1 extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root_nohisteq = Path('..','data','images_preprocessed','images_resize')\n",
    "assert img_root_nohisteq.is_dir()\n",
    "files_noh = sorted(img_root_nohisteq.glob('*'))\n",
    "rs = np.random.RandomState(seed=3626210179)\n",
    "rs.shuffle(files_noh)\n",
    "labels_noh = extract_labels(files_noh)\n",
    "assert len(files_noh) == 1800\n",
    "print('first 5 filenames and labels')\n",
    "print([x.name for x in files_noh[:5]])\n",
    "print(labels_noh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow the same process described above to load images, convert to array, and format for vgg16\n",
    "images_noh = load_images(files_noh)\n",
    "fc1_noh = fc1_extractor.predict(images_noh)\n",
    "\n",
    "results = {'filename': files_noh,\n",
    "          'features': fc1_noh,\n",
    "          'labels': labels_noh,\n",
    "          'layer_name': 'fc1 no_histeq'}\n",
    "with open(feature_dir / 'VGG16_fc1_features_NoHistEQ.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
