{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Model\n",
    "So far, we have fit the clusters to all of the data and then reported the accuuracy. \n",
    "\n",
    "But what if we wanted to use this model in a real QC setting, where new data is always coming in?\n",
    "\n",
    "To simulate this we will split the data into a training set, which we will fit the model to,\n",
    "\n",
    "and then a separate test set, to see how well the performance generalizes to new data.\n",
    "\n",
    "Note that this is not possible when clustering t-SNE projections (which is sometimes done in the literature), \n",
    "\n",
    "which require all data before computing the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module imports, data loading, and verifying PCA/t-SNE, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.manifold\n",
    "import sklearn.model_selection\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from helper import (classification_tools as ct,\n",
    "                   visualize as vis)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_path = Path('..','data','features','VGG16_fc1_features_std.pickle')\n",
    "assert fc1_path.is_file()\n",
    "\n",
    "le_path = Path('..','models','label_encoder.pickle')\n",
    "assert le_path.is_file()\n",
    "\n",
    "\n",
    "with open(fc1_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open(le_path, 'rb') as f:\n",
    "    le = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = data['filename']\n",
    "fc1 = data['features']\n",
    "labels = data['labels']\n",
    "y_gt = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pca=50\n",
    "pca = PCA(n_components=n_pca, svd_solver='full', whiten=True)\n",
    "pca_nw = PCA(n_components=n_pca, svd_solver='full', whiten=False)\n",
    "x = pca.fit_transform(fc1)\n",
    "x_nw = pca_nw.fit_transform(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(1,n_pca+1), pca_nw.explained_variance_ratio_.cumsum(), '-o')\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Cumulative Variance Explained')\n",
    "ax.grid(which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation\n",
    "The data is split into k-folds (in this case, 5.) 4 folds are used for training, and the remaining fold is held out for testing. \n",
    "\n",
    "The process is repeated such that each fold is used for validation once. This way we can see the performance on 5 trials, instead of just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "\n",
    "kmeans_cross_val_results_cache_path_50c = Path('..','.neu_cache','kmeans_crossval_cache_50c.pickle')\n",
    "\n",
    "if not use_cache or not kmeans_cross_val_results_cache_path_50c.is_file():\n",
    "    rs = np.random.RandomState(seed=3732236083)\n",
    "    n_splits=5\n",
    "    train_acc = np.zeros(n_splits)\n",
    "    val_acc = np.zeros(n_splits)\n",
    "    \n",
    "    kfold = sklearn.model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=rs.randint(2**32))\n",
    "    splits = kfold.split(x)\n",
    "\n",
    "    for i, (train, test) in enumerate(splits):\n",
    "        \n",
    "        fc1train = fc1[train]\n",
    "        fc1val = fc1[test]\n",
    "\n",
    "        Ytrain = y_gt[train]\n",
    "        Yval = y_gt[test]\n",
    "        \n",
    "        pca_cval = PCA(n_components=50, whiten=True, svd_solver='full')\n",
    "        pca_cval.fit(fc1train)\n",
    "        xtrain_cval = pca_cval.transform(fc1train)\n",
    "        xval_cval = pca_cval.transform(fc1val)\n",
    "        \n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=7, init='k-means++', n_init=500, random_state=rs.randint(2**32))\n",
    "        kmeans.fit(xtrain_cval)\n",
    "\n",
    "        labels_train = kmeans.labels_\n",
    "        labels_val = kmeans.predict(xval_cval)\n",
    "\n",
    "        Ypred_train = ct.label_matcher(labels_train, Ytrain)\n",
    "        Ypred_val = ct.label_matcher(labels_val, Yval)\n",
    "\n",
    "        train_acc[i] = (Ypred_train == Ytrain).sum()/len(Ytrain)\n",
    "        val_acc[i] = (Ypred_val == Yval).sum()/len(Yval)\n",
    "        \n",
    "    with open(kmeans_cross_val_results_cache_path_50c, 'wb') as f:\n",
    "        pickle.dump({'train_acc': train_acc,\n",
    "                    'val_acc': val_acc,\n",
    "                    'n_pca':n_pca},\n",
    "                   f)\n",
    "else:\n",
    "    with open(kmeans_cross_val_results_cache_path_50c, 'rb') as f:\n",
    "        results_ = pickle.load(f)\n",
    "        train_acc = results_['train_acc']\n",
    "        val_acc = results_['val_acc']\n",
    "        n_pca = results_['n_pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCA {:>3} components + whitening'.format(50))\n",
    "print('Train Accuracy\\t\\tValidation Accuracy')\n",
    "for row in zip(train_acc, val_acc):\n",
    "    print(\"    {:.4f}\\t\\t\\t{:.4f}\".format(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we do better?\n",
    "The model gets 96-97\\% accuracy on the test data, which is lower than the 99+\\% we saw for the standard analysis.\n",
    "\n",
    "Let's try reducing the number of PCA components. Since there are fewer images in the training set, fewer components can be fit without including noisy components.\n",
    "\n",
    "I tried a couple and found that 35 works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.cumsum()[[34, 49]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "kmeans_cross_val_results_cache_path = Path('..','.neu_cache','kmeans_crossval_cache.pickle')\n",
    "\n",
    "if not use_cache or not kmeans_cross_val_results_cache_path.is_file():\n",
    "    rs = np.random.RandomState(seed=2618930764)\n",
    "    n_splits=5\n",
    "    train_acc = np.zeros(n_splits)\n",
    "    val_acc = np.zeros(n_splits)\n",
    "    \n",
    "    kfold = sklearn.model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=rs.randint(2**32))\n",
    "    splits = kfold.split(x)\n",
    "    n_pca=35\n",
    "\n",
    "    for i, (train, test) in enumerate(splits):\n",
    "        \n",
    "        fc1train = fc1[train]\n",
    "        fc1val = fc1[test]\n",
    "\n",
    "        Ytrain = y_gt[train]\n",
    "        Yval = y_gt[test]\n",
    "        \n",
    "        pca_cval = PCA(n_components=n_pca, whiten=True, svd_solver='full')\n",
    "        pca_cval.fit(fc1train)\n",
    "        xtrain_cval = pca_cval.transform(fc1train)\n",
    "        xval_cval = pca_cval.transform(fc1val)\n",
    "        \n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=7, init='k-means++', n_init=500, random_state=rs.randint(2**32))\n",
    "        kmeans.fit(xtrain_cval)\n",
    "\n",
    "        labels_train = kmeans.labels_\n",
    "        labels_val = kmeans.predict(xval_cval)\n",
    "\n",
    "        Ypred_train = ct.label_matcher(labels_train, Ytrain)\n",
    "        Ypred_val = ct.label_matcher(labels_val, Yval)\n",
    "\n",
    "        train_acc[i] = (Ypred_train == Ytrain).sum()/len(Ytrain)\n",
    "        val_acc[i] = (Ypred_val == Yval).sum()/len(Yval)\n",
    "        \n",
    "    with open(kmeans_cross_val_results_cache_path, 'wb') as f:\n",
    "        pickle.dump({'train_acc': train_acc,\n",
    "                    'val_acc': val_acc,\n",
    "                    'n_pca':n_pca},\n",
    "                   f)\n",
    "else:\n",
    "    with open(kmeans_cross_val_results_cache_path, 'rb') as f:\n",
    "        results_ = pickle.load(f)\n",
    "        train_acc = results_['train_acc']\n",
    "        val_acc = results_['val_acc']\n",
    "        n_pca = results_['n_pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCA {:>3} components + whitening'.format(n_pca))\n",
    "print('Train Accuracy\\t\\tValidation Accuracy')\n",
    "for i, (trainacc, valacc) in enumerate(zip(train_acc, val_acc)):\n",
    "    print(\"    {:.4f}\\t\\t\\t{:.4f}\".format(trainacc, valacc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All but 1 of the validation folds achieve 99+\\% accuracy. Pretty good!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
