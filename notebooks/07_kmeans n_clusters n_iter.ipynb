{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity study: K-means\n",
    "How do the settings used when running K-means affect the classification performance?\n",
    "\n",
    "We will vary 2 parameters:\n",
    "  * Number of initializations\n",
    "  * Number of cluster centers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import skimage\n",
    "from skimage.exposure import histogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from helper import (classification_tools as ct,\n",
    "                   visualize as vis)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_path = Path('..','data','features','VGG16_fc1_features_std.pickle')\n",
    "assert fc1_path.is_file()\n",
    "\n",
    "le_path = Path('..','models','label_encoder.pickle')\n",
    "assert le_path.is_file()\n",
    "\n",
    "\n",
    "with open(fc1_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open(le_path, 'rb') as f:\n",
    "    le = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = data['filename']\n",
    "fc1 = data['features']\n",
    "labels = data['labels']\n",
    "y_gt = le.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and t-SNE visualization\n",
    "verify that everything is loaded correctly before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50, svd_solver='full', whiten=True)\n",
    "pca_nw = PCA(n_components=50, svd_solver='full', whiten=False)\n",
    "x = pca.fit_transform(fc1)\n",
    "x_nw = pca_nw.fit_transform(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=12214)\n",
    "x_nw_tsne = tsne.fit_transform(x_nw)\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x_nw_tsne[:,0], x_nw_tsne[:,1], hue=labels, hue_order=le.labels_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of number of initializations on cluster performance\n",
    "We will run k-means 5000 times, each with a single initialization step, and track the accuracy and inertia of each trial. The process will be repeated for clustering both whitened and unwhitened PCA components, which will reveal an important trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tests=5000\n",
    "rs = np.random.RandomState(seed=3688757485)\n",
    "rs_w = rs.randint(2**32, size=n_tests)\n",
    "rs_nw = rs.randint(2**32, size=n_tests)\n",
    "\n",
    "use_cache=True\n",
    "kmeans_acc_vs_inertia_cache_path = Path('..','.neu_cache','kmeans_acc_vs_inertia.pickle')\n",
    "\n",
    "if not use_cache or not kmeans_acc_vs_inertia_cache_path.is_file():\n",
    "    accs_w = np.zeros(n_tests)\n",
    "    inertias_w = np.zeros(n_tests)\n",
    "    \n",
    "    accs_nw = np.zeros(n_tests)\n",
    "    inertias_nw = np.zeros(n_tests)\n",
    "    for i, (seed_w, seed_nw) in enumerate(zip(rs_w, rs_nw)):\n",
    "        \n",
    "        # cluster results for each random seed, storing the  accuracy and inertia\n",
    "        kmeans = KMeans(n_clusters=7, init='k-means++', n_init=1, random_state=seed_w)\n",
    "        kmeans.fit(x)\n",
    "        y_pred = ct.label_matcher(kmeans.labels_, y_gt)\n",
    "        CM = confusion_matrix(y_gt, y_pred)\n",
    "        accs_w[i] = CM.trace()/CM.sum()\n",
    "        inertias_w[i] = kmeans.inertia_\n",
    "        \n",
    "        # repeat for unwhitened components\n",
    "        kmeans_nw = KMeans(n_clusters=7, init='k-means++', n_init=1, random_state=seed_nw)\n",
    "        kmeans_nw.fit(x_nw)\n",
    "        y_pred_nw = ct.label_matcher(kmeans_nw.labels_, y_gt)\n",
    "        CM_nw = confusion_matrix(y_gt, y_pred_nw)\n",
    "        accs_nw[i] = CM_nw.trace()/CM_nw.sum()\n",
    "        inertias_nw[i] = kmeans_nw.inertia_\n",
    "    with open(kmeans_acc_vs_inertia_cache_path, 'wb') as f:\n",
    "        pickle.dump({'accs_w': accs_w,\n",
    "                     'accs_nw': accs_nw,\n",
    "                     'inertias_w': inertias_w,\n",
    "                     'inertias_nw': inertias_nw},\n",
    "                    f)\n",
    "else:\n",
    "    with open(kmeans_acc_vs_inertia_cache_path, 'rb') as f:\n",
    "        results_ = pickle.load(f)\n",
    "        accs_nw = results_['accs_nw']\n",
    "        accs_w = results_['accs_w']\n",
    "        inertias_nw = results_['inertias_nw']\n",
    "        inertias_w = results_['inertias_w']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_hist(data, bins):\n",
    "    \"\"\"\n",
    "    Computes histogram with specified number of bins.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: ndarray\n",
    "        n-element array of data from which to compute histogram\n",
    "    \n",
    "    bins: int\n",
    "        number of bins to use in histogram\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    hist: ndarray\n",
    "        *bins* element array of counts from the histogram\n",
    "    \n",
    "    bin_centers: ndarray\n",
    "        *bins* element array where each item is the center of the bin in the histogram \n",
    "    \n",
    "    \"\"\"\n",
    "    hist, bin_edges = np.histogram(data, bins=bins)\n",
    "    bin_centers = np.asarray([np.mean([x,y]) for x,y in zip(bin_edges[:-1], bin_edges[1:])])\n",
    "    \n",
    "    return hist, bin_centers\n",
    "    \n",
    "hist_w, bins_w = format_hist(accs_w, bins=20)\n",
    "\n",
    "hist_nw, bins_nw = format_hist(accs_nw, bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize=(6,3.5), dpi=300,facecolor='w')\n",
    "\n",
    "ax = axs[0,0]\n",
    "ax.plot(inertias_nw/inertias_nw.max(), accs_nw, 'o', color='slateblue')\n",
    "\n",
    "ax = axs[0,1]\n",
    "ax.bar(bins_nw, hist_nw/hist_nw.sum(), width=0.05, color='slateblue')\n",
    "ax.set_xlim([0.35, 1.05])\n",
    "\n",
    "ax.set_yticks(np.linspace(0,1,5), minor=True)\n",
    "\n",
    "ax = axs[1,0]\n",
    "ax.plot(inertias_w/inertias_w.max(), accs_w, 'o', color='slateblue')\n",
    "ax.set_xlabel('relative inertia')\n",
    "\n",
    "\n",
    "ax = axs[1,1]\n",
    "ax.bar(bins_w, hist_w/hist_w.sum(), width=0.05, color='slateblue')\n",
    "ax.set_xlim([0.35, 1.05])\n",
    "ax.set_yticks(np.linspace(0,0.2,5), minor=True)\n",
    "ax.set_xlabel('accuracy')\n",
    "\n",
    "\n",
    "for a in axs[:,1].ravel():\n",
    "    a.set_ylabel('frequency')\n",
    "for a in axs[:,0].ravel():\n",
    "    a.set_ylabel('accuracy')\n",
    "\n",
    "for a, title in zip(axs.ravel(), ['a','b','c','d']):\n",
    "    a.set_title(\"{})\".format(title), loc='left', **{'x':0.9,'y':0.78})\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(Path('..','Figures','kmeans_n_iter.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trends are covered in detail in the paper, but the main points are as follows:\n",
    "  * Without whitening, there are a relatively small number of final cluster positions. Most of the time, even with a single initialiation step, the model reaches 96\\% accuarcy.\n",
    "  * With whitening, there are a lot of local minima that the centroids can become trapped in. On average the model achives 80\\% accuracy for a single run. However, \n",
    "    accuracy tends to increase as the inertia decreases. Thus, running many initializations and taking the result with the lowest inertia allows us to consistently reach 99+\\% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No Whitening')\n",
    "print('\\tmin acc: \\t{:.3f}\\n\\tmax acc:\\t{:.3f}\\nacc-min inertia:\\t{:.3f}\\n'.format(\n",
    "    accs_nw.min(), accs_nw.max(), accs_nw[inertias_nw.argmin()]))\n",
    "\n",
    "print('With Whitening')\n",
    "print('\\tmin acc: \\t{:.3f}\\n\\tmax acc:\\t{:.3f}\\nacc-min inertia:\\t{:.3f}'.format(\n",
    "    accs_w.min(), accs_w.max(), accs_w[inertias_w.argmin()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No whitening')\n",
    "print('\\ttop bin accuracy:\\t{:.3f}\\n\\ttop bin fraction:\\t{:.3f}\\n\\n'\n",
    "      .format(bins_nw[-2], hist_nw[-1]/hist_nw.sum()))\n",
    "bins_nw[-2]\n",
    "\n",
    "print('With whitening')\n",
    "print('\\ttop bin accuracy:\\t{:.3f}\\n\\ttop bin fraction:\\t{:.3f}'\n",
    "      .format(0.99, (accs_w > 0.99).sum()/len(accs_w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_w[hist_w.argmax():hist_w.argmax()+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pearson r correlation between accuracy and inertia')\n",
    "print('whitening: \\t{:.3f}'.format(pearsonr(inertias_w, accs_w)[0]))\n",
    "print('no whitening:\\t{:.3f}'.format(pearsonr(inertias_nw, accs_nw)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of number of clusters on accuracy\n",
    "We already saw that changing the number of clusters can drastically affect the performance during other steps of the sensitivity analysis.\n",
    "What about for the original fc1 features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "kmeans_k_cache_path = Path('..','.neu_cache','kmeans_acc_vs_k.pickle')\n",
    "\n",
    "if not use_cache or not kmeans_k_cache_path.is_file():\n",
    "    k_values = np.arange(6,26)\n",
    "    acc_k = np.zeros(k_values.shape)\n",
    "    rs = np.random.RandomState(seed=3689319368)\n",
    "    \n",
    "    \n",
    "    for i, (k, state) in enumerate(zip(k_values, rs.randint(2**32,size=len(k_values)))):\n",
    "\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', n_init=500, random_state=state)\n",
    "        kmeans.fit(x)\n",
    "        y_pred = ct.label_matcher(kmeans.labels_, y_gt)\n",
    "        CM = confusion_matrix(y_gt, y_pred)\n",
    "        acc_k[i] = CM.trace()/CM.sum()\n",
    "    \n",
    "    with open(kmeans_k_cache_path, 'wb') as f:\n",
    "        pickle.dump({'k_values': k_values,\n",
    "                    'acc_k': acc_k},\n",
    "                    f)\n",
    "else:\n",
    "    with open(kmeans_k_cache_path, 'rb') as f:\n",
    "        results_ = pickle.load(f)\n",
    "        k_values = results_['k_values']\n",
    "        acc_k = results_['acc_k']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=300, figsize=(3,2))\n",
    "ax.plot(k_values, acc_k, '-bo')\n",
    "#ax[1].plot(K, [np.median(x) for x in silhouettes], ':o')\n",
    "ax.set_xticks(range(5,26,5))\n",
    "ax.set_xlabel('number of clusters')\n",
    "ax.set_ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 7 clusters results in significantly higher accuracy than any other number between 6 and 25. Interestingly, using 6 clusters, equal to the number of defects, results in the lowest accuracy. This highlights\n",
    "the effect that splitting scratches into 2 clusters has.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2 = np.random.RandomState(seed=3689319368)\n",
    "k_ = np.arange(6,25)\n",
    "states = rs2.randint(2**32, size=k_.shape)\n",
    "\n",
    "i = 8\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=k_[i], init='k-means++', n_init=500, random_state=states[i])\n",
    "kmeans.fit(x)\n",
    "y_pred = ct.label_matcher(kmeans.labels_, y_gt)\n",
    "CM = confusion_matrix(y_gt, y_pred)\n",
    "acc = CM.trace()/CM.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mapper = {}\n",
    "for p in np.unique(y_pred):\n",
    "    y_clusters = kmeans.labels_[y_pred == p]\n",
    "    for idx, value in enumerate(np.unique(y_clusters)):\n",
    "        cluster_mapper[value] = '{}-{}'.format(le.inverse_transform([p])[0], idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = np.concatenate((sns.color_palette('pastel', 7), sns.color_palette('dark', 7)), axis=0)\n",
    "hue = [cluster_mapper[x] for x in kmeans.labels_]\n",
    "hue_order=sorted(cluster_mapper.values(), key=lambda x: x.upper())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(dpi=300, figsize=(5,5))\n",
    "sns.scatterplot(x_nw_tsne[:,0], x_nw_tsne[:,1], hue=hue, hue_order=hue_order, palette=dict(zip(hue_order, palette)), ax=ax)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are too many centroids, clusters get split up, leaving regions that are between the main clusters that get misclassified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
